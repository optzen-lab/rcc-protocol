# References

This page summarizes key literature and resources relevant to the RCC/PRT protocol.  
This repository itself is a provisional protocol based on observational logs, with the following works providing conceptual and theoretical background.  

---

## 1. Large Language Models (LLMs)
- Vaswani, A. et al. (2017). *Attention Is All You Need*. NeurIPS.  
- Brown, T. et al. (2020). *Language Models are Few-Shot Learners*. NeurIPS.  
- OpenAI (2023â€“2025). GPT-4 / GPT-4o / GPT-5 technical reports.  

---

## 2. Semantic Vectors and Meaning Space
- Mikolov, T. et al. (2013). *Efficient Estimation of Word Representations in Vector Space*. arXiv.  
- Pennington, J. et al. (2014). *GloVe: Global Vectors for Word Representation*. EMNLP.  
- Devlin, J. et al. (2018). *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*. NAACL.  

---

## 3. Attractors and Dynamical Systems
- Lorenz, E. N. (1963). *Deterministic Nonperiodic Flow*. J. Atmos. Sci.  
- Strogatz, S. (2014). *Nonlinear Dynamics and Chaos* (2nd Edition). CRC Press.  
- Kelso, J. A. S. (1995). *Dynamic Patterns: The Self-Organization of Brain and Behavior*. MIT Press.  

---

## 4. Conversation Research and HCI
- Clark, H. H. (1996). *Using Language*. Cambridge University Press.  
- Garrod, S., & Pickering, M. J. (2004). *Toward a mechanistic psychology of dialogue*. Behavioral and Brain Sciences.  
- Brennan, S. E. (1990). *Conversation as Direct Manipulation: An Iconic Interface*.  

---

## 5. Original Observations on Resonance and Conversational Convergence
- Takahashi, Y. (2025). *RCC/PRT Protocol v0.3.0-draft*. OPTZEN (GitHub Pages).  
- (Includes log records and training protocols from this repository)  

---

## 6. Recommended Citation
  Takahashi, Y. (2025). *RCC/PRT Protocol v0.3.0-draft*. OPTZEN (GitHub Pages).  

---
